{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d6f8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open(\"names.txt\", 'r').read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7fb524",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = {}\n",
    "for w in words:\n",
    "    # Add start and ending characters\n",
    "    chs = [\"<S>\"] + list(w) + [\"<E>\"] \n",
    "    # Get pairs of characters in a sliding window and add to dict\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        bigram = (ch1, ch2)\n",
    "        b[bigram] = b.get(bigram, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb0ebb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by highest occurence\n",
    "sorted(b.items(), key = lambda kv: kv[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e364cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59291f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = torch.zeros((27, 27), dtype = torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d75f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all letters of the alphabet in a list\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "# Create mapping\n",
    "s_to_i = {s:i for i, s in enumerate(chars, start=1)}\n",
    "s_to_i['.'] = 0\n",
    "s_to_i\n",
    "\n",
    "# Reverse Mapping\n",
    "i_to_s = {i:s for s, i in s_to_i.items()}\n",
    "i_to_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a1c23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in words:\n",
    "    # Add start and ending characters\n",
    "    chs = [\".\"] + list(w) + [\".\"] \n",
    "    # Get pairs of characters in a sliding window and add to dict\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = s_to_i[ch1]\n",
    "        ix2 = s_to_i[ch2]\n",
    "        N[ix1, ix2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5de18d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(N, cmap='Blues')\n",
    "\n",
    "for i in range(27):\n",
    "    for j in range(27):\n",
    "        chstr = i_to_s[i] + i_to_s[j]\n",
    "        plt.text(j, i, chstr, ha=\"center\", va=\"bottom\", color=\"gray\", fontsize='4')\n",
    "        plt.text(j, i, N[i, j].item(), ha=\"center\", va=\"top\", color=\"gray\", fontsize='4')\n",
    "\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaecefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do +1 to smooth the model and prevent -inf prob from log\n",
    "P = (N+1).float()\n",
    "# Sum each row of the vector, and return sum in column (1) format\n",
    "# Keep dim is True to hold dimensions and will be used for broadcasting\n",
    "P /= P.sum(1, keepdim=True) # /= is faster because in place and doesn't create new memory\n",
    "P[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb15e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator for multimonial\n",
    "g = torch.Generator().manual_seed(214748)\n",
    "\n",
    "for i in range(10):\n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "\n",
    "        p = P[ix] # Made code below more efficient on top\n",
    "        # p = N[ix].float()\n",
    "        # Get prob for each p (first letter)\n",
    "        # p = p / p.sum()\n",
    "        # print(p)\n",
    "        \n",
    "        # draw random samples from a dataset where some items are more likely to be picked than others\n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(i_to_s[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53abf712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOAL: maximise the likelihood of data w.r.t. model parameters\n",
    "# equivalent to maximising log likelihood\n",
    "# equivalent to minimising negative log likelihood\n",
    "# equivalent to minmising the average nll \n",
    "\n",
    "# We use log because log(a * b * c) = log(a) + log(b) + log(c)\n",
    "\n",
    "log_likelihood = 0.0\n",
    "n = 0\n",
    "\n",
    "for w in words:\n",
    "    # Add start and ending characters\n",
    "    chs = [\".\"] + list(w) + [\".\"] \n",
    "    # Get pairs of characters in a sliding window and add to dict\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = s_to_i[ch1]\n",
    "        ix2 = s_to_i[ch2]\n",
    "        prob = P[ix1, ix2]\n",
    "        logprob = torch.log(prob)\n",
    "        log_likelihood += logprob\n",
    "        n += 1\n",
    "        print(f'{ch1}{ch2}: {prob:.4f}{logprob:.4f}')\n",
    "\n",
    "print(f'{log_likelihood=}')\n",
    "nll = -log_likelihood\n",
    "print(f'{nll=}')\n",
    "print(f'{nll/n=}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d21893",
   "metadata": {},
   "source": [
    "## Full Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46470112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training set for the bigrams (x,y)\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words:\n",
    "    # Add start and ending characters\n",
    "    chs = [\".\"] + list(w) + [\".\"] \n",
    "    # Get pairs of characters in a sliding window and add to list\n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = s_to_i[ch1]\n",
    "        ix2 = s_to_i[ch2]\n",
    "        print(ch1, ch2)\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbdb46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# initialise the network - randn is a standard normal distribution with mean 0 and variance 1\n",
    "W = torch.randn(27, 27, requires_grad=True)\n",
    "\n",
    "for k in range(100):\n",
    "    # -- forward pass\n",
    "    # creates a vector for each letter, index of char = 1, rest = 0\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = (xenc @ W) # predicts log counts\n",
    "    # this creates the softmax function\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdim=True) # probs for next character\n",
    "    loss = -probs[torch.arange(num), ys].log().mean() + 0.1 * (W**2).mean() # applying regularisation - way to penalise using large weights\n",
    "\n",
    "    # -- backward pass\n",
    "    W.grad = None # set to zero\n",
    "    loss.backward()\n",
    "\n",
    "    # -- update\n",
    "    W.data += -50 * W.grad \n",
    "    print(loss)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
