{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee4723b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cd22ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all letters of the alphabet in a list\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "\n",
    "# Create mapping\n",
    "s_to_i = {s:i for i, s in enumerate(chars, start=1)}\n",
    "s_to_i['.'] = 0\n",
    "\n",
    "# Reverse Mapping\n",
    "i_to_s = {i:s for s, i in s_to_i.items()}\n",
    "i_to_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7b616b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182437, 3]) torch.Size([182437])\n",
      "torch.Size([22781, 3]) torch.Size([22781])\n",
      "torch.Size([22928, 3]) torch.Size([22928])\n"
     ]
    }
   ],
   "source": [
    "# Build the dataset\n",
    "\n",
    "def build_dataset(words):\n",
    "    block_size = 3\n",
    "    X, Y = [], []\n",
    "\n",
    "    for w in words:\n",
    "\n",
    "        context = [0] * block_size\n",
    "        for ch in w + '.':\n",
    "            ix = s_to_i[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            # print(''.join(i_to_s[i] for i in context), '--->', i_to_s[ix])\n",
    "            context = context[1:] + [ix]\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1])\n",
    "Xdev, Ydev = build_dataset(words[n1:n2])\n",
    "Xte, Yte = build_dataset(words[n2:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f1dffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2))\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4464d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(C.shape, X.shape, C[X].shape)\n",
    "\n",
    "# C = (27, 2)    -> Lookup Table: 27 characters, each has a 2D vector\n",
    "# X = (32, 3)    -> Input Batch: 32 examples, each is a list of 3 indices\n",
    "# emb = C[X]     -> (32, 3, 2): Every single index in X is replaced by its 2D vector\n",
    "\n",
    "emb = C[X]\n",
    "\n",
    "W1 = torch.randn((6, 100))\n",
    "b1 = torch.randn(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3083ecc2",
   "metadata": {},
   "source": [
    "### Note to self\n",
    "We can extend this \"chain\" in two major directions. In Deep Learning, we actually give these dimensions standard names: B, T, and C.\n",
    "\n",
    "The current shape is (32, 3, 2). Let's translate that:\n",
    "\n",
    "* B (Batch) = 32: The stack of parallel examples.\n",
    "\n",
    "* T (Time) = 3: The length of the chain (sequence length).\n",
    "\n",
    "* C (Channels) = 2: The depth of information per link (embedding size).\n",
    "\n",
    "**Extending the Chain (Increasing T)**\n",
    "\n",
    "Can we chain \"a to b to c to d\". Yes. This is just increasing the Context Length (or Block Size).\n",
    "\n",
    "* If we look back 3 characters: (32, 3, 2)\n",
    "\n",
    "* If we look back 10 characters: (32, 10, 2)\n",
    "\n",
    "* If we look back 100 characters: (32, 100, 2)\n",
    "\n",
    "The \"chain\" of characters gets longer, so the tensor gets wider.\n",
    "\n",
    "**Extending the Depth (Increasing C)**\n",
    "   \n",
    "We can also make the \"coordinates\" richer. Right now, our characters live in 2D space (2 numbers).\n",
    "\n",
    "In real models (like GPT), characters live in 768-dimensional space or more.\n",
    "\n",
    "Shape: (32, 3, 768)\n",
    "\n",
    "This means every single character is described by a list of 768 numbers, not just 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3bd32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we want to turn (32, 3, 2) into (32, 6) so we can pass it into the hidden layer (6, 100)\n",
    "# We can do this using unbind on the emb on the character level dim (3) and concat with coordinates dim (2)\n",
    "# This results in the flattening of 3x2 into (32,6)\n",
    "\n",
    "# HOWEVER: this is inefficient because concat creates new memory vs manipulating the tensor view\n",
    "# --> torch.cat(torch.unbind(emb, 1), 1).shape\n",
    "\n",
    "# Instead we manipulate the view\n",
    "# emb.view(-1/emb.shape[0], 6) == torch.cat(torch.unbind(emb, 1), 1)\n",
    "\n",
    "h_pre = emb.view(-1, 6) @ W1 + b1 # h.shape = (32,100), w1 shape (6, 100), b.shape (100) -> this is broadcasted\n",
    "h = torch.tanh(h_pre) # hidden layers of activations using tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8945c5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W2 = torch.randn((100, 27))\n",
    "b2 = torch.randn(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41db187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2\n",
    "\n",
    "# Now we want to make logits positive\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdim=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fecfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Avg negative log likelihood\n",
    "loss = -probs[torch.arange(X.shape[0]), Y].log().mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909fe0d6",
   "metadata": {},
   "source": [
    "## Full Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "59424e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "C = torch.randn((27, 5), generator=g)\n",
    "W1 = torch.randn((15, 200), generator=g)\n",
    "b1 = torch.randn((200), generator=g)\n",
    "W2 = torch.randn((200, 27), generator=g)\n",
    "b2 = torch.randn((27), generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "98d83704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8762"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of params in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "1cf3a417",
   "metadata": {},
   "outputs": [],
   "source": [
    "lre = torch.linspace(-3, 0, 1000)\n",
    "lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "8092b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "lri = []\n",
    "lossi = []\n",
    "stepi = []\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "for i in range(50000):\n",
    "\n",
    "    # mini batch construct\n",
    "    ix = torch.randint(0, Xtr.shape[0], (32,))\n",
    "\n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]] # (32, 3, 2)\n",
    "    hidden_pre = emb.view(-1, 15) @ W1 + b1 # (32, 100)\n",
    "    hidden = torch.tanh(hidden_pre)\n",
    "    logits = hidden @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, Ytr[ix])\n",
    "    # print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # learning rate\n",
    "    lr = 0.01\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad\n",
    "\n",
    "    # track stats\n",
    "    # lri.append(lre[i])\n",
    "    stepi.append(i)\n",
    "    lossi.append(loss.log10().item())\n",
    "    \n",
    "# Much more efficient way using cross entropy\n",
    "# counts = logits.exp()\n",
    "# probs = counts / counts.sum(1, keepdim = True)\n",
    "# loss = -probs[torch.arange(X.shape[0]), Y].log().mean()\n",
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "9b265f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5976619720458984\n"
     ]
    }
   ],
   "source": [
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "f69a7a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4012, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xtr] # (32, 3, 2)\n",
    "hidden_pre = emb.view(-1, 15) @ W1 + b1 # (32, 100)\n",
    "hidden = torch.tanh(hidden_pre)\n",
    "logits = hidden @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "e3525b55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4146, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[Xdev] # (32, 3, 2)\n",
    "hidden_pre = emb.view(-1, 15) @ W1 + b1 # (32, 100)\n",
    "hidden = torch.tanh(hidden_pre)\n",
    "logits = hidden @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "f995f1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(lri, lossi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7442217",
   "metadata": {},
   "source": [
    "## Why mini batch\n",
    "\n",
    "**Why we trade perfect accuracy for speed.**\n",
    "\n",
    "Mini-batching is one of the most critical practical concepts in deep learning. Itâ€™s the difference between waiting **5 days** for a result versus **5 minutes**.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. The Problem: \"The Classroom Analogy\"\n",
    "\n",
    "Imagine you are a **Teacher** (the Neural Net) trying to improve your **Curriculum** (Weights). You have **30,000 Students** (The Dataset).\n",
    "\n",
    "#### Option A: Full Batch (Standard Gradient Descent)\n",
    "\n",
    "* **Method:** You grade the exam of **every single student** (all 30,000). You calculate the average mistake. You make **one** tiny adjustment to your teaching plan.\n",
    "* **Result:** This is painfully slow. You only improve once per semester.\n",
    "\n",
    "#### Option B: Stochastic Gradient Descent (SGD)\n",
    "\n",
    "* **Method:** You grade **one random student**. You change your *entire* curriculum based on that one kid.\n",
    "* **Result:** **Chaos.** If that kid was a genius or totally lost, you make wild, noisy adjustments. You zigzag everywhere.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. The Solution: Mini-Batching (The Sweet Spot)\n",
    "\n",
    "#### Batch Size: 32\n",
    "\n",
    "* **Method:** You grab a random group of **32 students**. You grade them and get their average score.\n",
    "* **The Logic:** The average of 32 random students is usually a pretty good estimate of the average of the whole school. It's not perfect, but it's **\"good enough\"** to know which direction to move.\n",
    "* **Result:** You can update your curriculum **1,000 times** in the time it takes the \"Full Batch\" teacher to update just once.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Why is it \"Better\"?\n",
    "\n",
    "It is a trade-off between **Accuracy** and **Speed**, where Speed wins.\n",
    "\n",
    "### Speed (The Main Reason)\n",
    "\n",
    "* Calculating gradients for 30,000 rows might take **1 second**.\n",
    "* Calculating gradients for 32 rows takes **0.001 seconds**.\n",
    "* **The Benefit:** You can take many more steps down the hill. Even if the steps are slightly \"drunk\" (approximate), taking **10,000 steps** gets you to the bottom faster than taking 5 perfect steps.\n",
    "\n",
    "#### Hardware Efficiency\n",
    "\n",
    "* GPUs love matrices of size **32, 64, 128**. They are physically optimized to process these blocks in parallel.\n",
    "* Processing **1 item** is inefficient (overhead).\n",
    "* Processing **1,000,000 items** might crash your RAM.\n",
    "* **32 is the \"Goldilocks\" zone.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.14)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
